<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Inteceptor Robot: Final report | Interceptor</title>
<meta name=keywords content>
<meta name=description content="Introduction Interceptor is a robot that can stop objects from rolling off a table. It detects a moving tennis ball, predicts where the ball will roll off the table, then moves its gripper to cut off the ball’s path.
Our application shows how robots can quickly predict outcomes of physical situations and respond to them. Potential applications may be ensuring safety in environments where robots work (e.g. if a part goes flying, can the robot stop it from hitting a human?">
<meta name=author content="Robert Peltekov, Saahil Parikh, Ryan Adolf">
<link rel=canonical href=http://example.org/>
<link crossorigin=anonymous href=/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=http://example.org/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=http://example.org/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=http://example.org/favicon-32x32.png>
<link rel=apple-touch-icon href=http://example.org/apple-touch-icon.png>
<link rel=mask-icon href=http://example.org/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.91.0">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript><meta property="og:title" content="Inteceptor Robot: Final report">
<meta property="og:description" content="Introduction Interceptor is a robot that can stop objects from rolling off a table. It detects a moving tennis ball, predicts where the ball will roll off the table, then moves its gripper to cut off the ball’s path.
Our application shows how robots can quickly predict outcomes of physical situations and respond to them. Potential applications may be ensuring safety in environments where robots work (e.g. if a part goes flying, can the robot stop it from hitting a human?">
<meta property="og:type" content="article">
<meta property="og:url" content="http://example.org/"><meta property="article:section" content>
<meta property="article:published_time" content="2021-12-17T20:36:16-08:00">
<meta property="article:modified_time" content="2021-12-17T20:36:16-08:00">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Inteceptor Robot: Final report">
<meta name=twitter:description content="Introduction Interceptor is a robot that can stop objects from rolling off a table. It detects a moving tennis ball, predicts where the ball will roll off the table, then moves its gripper to cut off the ball’s path.
Our application shows how robots can quickly predict outcomes of physical situations and respond to them. Potential applications may be ensuring safety in environments where robots work (e.g. if a part goes flying, can the robot stop it from hitting a human?">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Inteceptor Robot: Final report","item":"http://example.org/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Inteceptor Robot: Final report","name":"Inteceptor Robot: Final report","description":"Introduction Interceptor is a robot that can stop objects from rolling off a table. It detects a moving tennis ball, predicts where the ball will roll off the table, then moves its gripper to cut off the ball’s path.\nOur application shows how robots can quickly predict outcomes of physical situations and respond to them. Potential applications may be ensuring safety in environments where robots work (e.g. if a part goes flying, can the robot stop it from hitting a human?","keywords":[],"articleBody":"   Introduction Interceptor is a robot that can stop objects from rolling off a table. It detects a moving tennis ball, predicts where the ball will roll off the table, then moves its gripper to cut off the ball’s path.\nOur application shows how robots can quickly predict outcomes of physical situations and respond to them. Potential applications may be ensuring safety in environments where robots work (e.g. if a part goes flying, can the robot stop it from hitting a human?) or working with humans (e.g. performing daycare for a baby that’s throwing its toys around).\nDesign Criteria The robot must be able to:\n Use AR tags to figure out how the camera, robot, and table are positioned relative to each other. Accurately detect the position of the tennis ball. Accurately determine the trajectory of the tennis ball, with minimal noise. Move its gripper to intercept the tennis ball, but avoid hitting the table. Perform steps 2-3 fast enough to intercept the ball before it rolls off.  Trade-offs  Vision system: We had to choose between making our position prediction fast versus making it accurate. We chose accuracy because any noise in the position data will make velocity data even noisier. So we chose to use a Kinect with point cloud data and process images at medium resolution, compared to faster solution like using a non-depth camera or sampling at lower resolution. Kalman filter: There’s an inherent tradeoff between exactly following input data and relying on the predicted state. We tuned the filter to closely fit the input data with a small amount of estimation since the ball movements can be erratic. Flexibility of motion planning: We chose to use the ROS MoveIt package to do motion planning for our arm. The package is flexible enough that it allows us to intercept the ball no matter what the height of the table is and also avoid hitting the table. However, MoveIt is slow and we considered replacing it with a custom solution like other teams have done. We evaluated using a hashmap to find joint positions, but this approach does not simultaneously meet our flexibility and obstacle avoidance requirements.  Implementation Hardware  Sawyer: Multi jointed arm robot Intel Realsense: Time of Flight Depth Camera Tennis Ball: Test object  Software  Our code: https://github.com/SaahilParikh/Interceptor  AR Alvar node We use two ar_track_alvar nodes for detecting AR tags on the table in the viewport of the depth camera and the robot: For the camera, we run a standard ar_track_alvar node. This node runs for the whole duration of the project, and the camera is always in view of the ar tag. For the robot, we modified a copy of the ar_track_alvar source to publish transforms from the Sawyer arm camera with different names. This node only runs for the initialization phase of operation where the arm is in position to see the non moving ar tag. After Initialization is over and the static transform of the ar tag to the robot is published, this node shuts down.\nTf Broadcaster node This node publishes a static transform from the ar tag to the base of the robot\nBall segment node Using OpenCV, the Ball Segment node publishes the current position of the ball relative to the constructed TF reference tree. The segmentation of the images happens in the following manner:\n Transform color RGB image from HSV (hue, saturation, value) Threshold filter for the tennis ball neon green yellow hue Use CV Erosion to smooth image and remove noise Use CV Contours to trace contours of isolations in image Publish an image with pixels filled only in the area of the largest contour Mask the depth points with the segmented ball image Calculate the center point from the resulting masked set of depth points Publish the center point in the space of the camera frame  Motion predict node Using the ball coordinates transformed relative to the reference frame AR tag on the table. The node uses a Kalman filter to smooth position data and predict the velocity. With the predicted velocity, the node determines where the ball will intersect the edge of the table.\nIK node Using the goal position from the motion predict node, we run ik using tracik to move sawyers gripper to the goal position. ik is implemented using move it which also allows us to implement obstacle avoidance to not hit the table or the side walls. The ik node waits for the goal position to update ( using std and stats to define update ) and then quickly moved sawyers multijointed arm. the ik solver uses distance to assist in the rtt search over the space.\nRealsense node We use Intel’s ROS package for the Realsense camera to publish point clouds and images from the depth camera. Additionally the realsense node takes care of the following functions: Unwarping the camera image using the camera matrix; Computing the color image projection onto the depth cloud and publishing the combined point cloud.\nMoveIt node The MoveIt node performs inverse kinematics and path planning, creating a series of steps of joint angles for the Sawyer to move to so that it reaches a desired end effector position without hitting the table.\nJoint trajectory action server node Not a node that we made. this node is to allow sawyer to know it’s joints?\nResults    The robot is able to successfully predict where the ball will fall from the table and move its gripper to this spot; our only major issue is the lag in the system, which causes the robot to move a few seconds after the ball has started rolling.\nIf we were to continue this project, we would implement a different inverse kinematics solver utilizing fewerr joints of the sawyer arm to more efficiently compute the IK problem and more easily follow a path within the constrained set of movements we need.\n   Team  Robert Peltekov\nA junior double majoring in EECS and Business at UC Berkeley. Love everything engineering-related from robotics, hardware design to space! In freetime I love to skate both in parks and down hills!    Saahil Parikh\nA third year eecs student. I’m a designer and creator. I love taking ideas from conception to fruition.    Ryan Adolf\nIt’s my third year at Berkeley majoring in EECS. I love engineering, design, art, and just making stuff in general!   ","wordCount":"1054","inLanguage":"en","datePublished":"2021-12-17T20:36:16-08:00","dateModified":"2021-12-17T20:36:16-08:00","author":[{"@type":"Person","name":"Robert Peltekov"},{"@type":"Person","name":"Saahil Parikh"},{"@type":"Person","name":"Ryan Adolf"}],"mainEntityOfPage":{"@type":"WebPage","@id":"http://example.org/"},"publisher":{"@type":"Organization","name":"Interceptor","logo":{"@type":"ImageObject","url":"http://example.org/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=http://example.org/ accesskey=h title="Interceptor (Alt + H)">Interceptor</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<h1 class=post-title>
Inteceptor Robot: Final report
</h1>
<div class=post-meta><span title="2021-12-17 20:36:16 -0800 -0800">December 17, 2021</span>&nbsp;·&nbsp;Robert Peltekov, Saahil Parikh, Ryan Adolf
</div>
</header>
<div class=post-content>
<div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden>
<iframe src=https://www.youtube.com/embed/a_u6m24022I style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe>
</div>
<h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1>
<p>Interceptor is a robot that can stop objects from rolling off a table. It detects a moving tennis ball, predicts where the ball will roll off the table, then moves its gripper to cut off the ball’s path.</p>
<p>Our application shows how robots can quickly predict outcomes of physical situations and respond to them. Potential applications may be ensuring safety in environments where robots work (e.g. if a part goes flying, can the robot stop it from hitting a human?) or working with humans (e.g. performing daycare for a baby that’s throwing its toys around).</p>
<h1 id=design-criteria>Design Criteria<a hidden class=anchor aria-hidden=true href=#design-criteria>#</a></h1>
<p>The robot must be able to:</p>
<ol>
<li>Use AR tags to figure out how the camera, robot, and table are positioned relative to each other.</li>
<li>Accurately detect the position of the tennis ball.</li>
<li>Accurately determine the trajectory of the tennis ball, with minimal noise.</li>
<li>Move its gripper to intercept the tennis ball, but avoid hitting the table.</li>
<li>Perform steps 2-3 fast enough to intercept the ball before it rolls off.</li>
</ol>
<h1 id=trade-offs>Trade-offs<a hidden class=anchor aria-hidden=true href=#trade-offs>#</a></h1>
<ul>
<li><strong>Vision system</strong>: We had to choose between making our position prediction fast versus making it accurate. We chose accuracy because any noise in the position data will make velocity data even noisier. So we chose to use a Kinect with point cloud data and process images at medium resolution, compared to faster solution like using a non-depth camera or sampling at lower resolution.</li>
<li><strong>Kalman filter</strong>: There’s an inherent tradeoff between exactly following input data and relying on the predicted state. We tuned the filter to closely fit the input data with a small amount of estimation since the ball movements can be erratic.</li>
<li><strong>Flexibility of motion planning</strong>: We chose to use the ROS MoveIt package to do motion planning for our arm. The package is flexible enough that it allows us to intercept the ball no matter what the height of the table is and also avoid hitting the table. However, MoveIt is slow and we considered replacing it with a custom solution like other teams have done. We evaluated using a hashmap to find joint positions, but this approach does not simultaneously meet our flexibility and obstacle avoidance requirements.</li>
</ul>
<h1 id=implementation>Implementation<a hidden class=anchor aria-hidden=true href=#implementation>#</a></h1>
<h2 id=hardware>Hardware<a hidden class=anchor aria-hidden=true href=#hardware>#</a></h2>
<ul>
<li>Sawyer: Multi jointed arm robot</li>
<li>Intel Realsense: Time of Flight Depth Camera</li>
<li>Tennis Ball: Test object</li>
</ul>
<h2 id=software>Software<a hidden class=anchor aria-hidden=true href=#software>#</a></h2>
<div style="background:var(--entry);padding:.5em 1em;border-radius:var(--radius);border:1px solid var(--border)"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="width:26px;ehight:26px;vertical-align:-20%;margin-right:.25em"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
Our code: <a href=https://github.com/SaahilParikh/Interceptor>https://github.com/SaahilParikh/Interceptor</a>
</div>
<h3 id=ar-alvar-node>AR Alvar node<a hidden class=anchor aria-hidden=true href=#ar-alvar-node>#</a></h3>
<p>We use two ar_track_alvar nodes for detecting AR tags on the table in the viewport of the depth camera and the robot:
For the camera, we run a standard ar_track_alvar node. This node runs for the whole duration of the project, and the camera is always in view of the ar tag.
For the robot, we modified a copy of the ar_track_alvar source to publish transforms from the Sawyer arm camera with different names. This node only runs for the initialization phase of operation where the arm is in position to see the non moving ar tag. After Initialization is over and the static transform of the ar tag to the robot is published, this node shuts down.</p>
<h3 id=tf-broadcaster-node>Tf Broadcaster node<a hidden class=anchor aria-hidden=true href=#tf-broadcaster-node>#</a></h3>
<p>This node publishes a static transform from the ar tag to the base of the robot</p>
<h3 id=ball-segment-node>Ball segment node<a hidden class=anchor aria-hidden=true href=#ball-segment-node>#</a></h3>
<p>Using OpenCV, the Ball Segment node publishes the current position of the ball relative to the constructed TF reference tree. The segmentation of the images happens in the following manner:</p>
<ol>
<li>Transform color RGB image from HSV (hue, saturation, value)</li>
<li>Threshold filter for the tennis ball neon green yellow hue</li>
<li>Use CV Erosion to smooth image and remove noise</li>
<li>Use CV Contours to trace contours of isolations in image</li>
<li>Publish an image with pixels filled only in the area of the largest contour</li>
<li>Mask the depth points with the segmented ball image</li>
<li>Calculate the center point from the resulting masked set of depth points</li>
<li>Publish the center point in the space of the camera frame</li>
</ol>
<h3 id=motion-predict-node>Motion predict node<a hidden class=anchor aria-hidden=true href=#motion-predict-node>#</a></h3>
<p>Using the ball coordinates transformed relative to the reference frame AR tag on the table. The node uses a Kalman filter to smooth position data and predict the velocity. With the predicted velocity, the node determines where the ball will intersect the edge of the table.</p>
<h3 id=ik-node>IK node<a hidden class=anchor aria-hidden=true href=#ik-node>#</a></h3>
<p>Using the goal position from the motion predict node, we run ik using tracik to move sawyers gripper to the goal position. ik is implemented using move it which also allows us to implement obstacle avoidance to not hit the table or the side walls. The ik node waits for the goal position to update ( using std and stats to define update ) and then quickly moved sawyers multijointed arm. the ik solver uses distance to assist in the rtt search over the space.</p>
<h3 id=realsense-node>Realsense node<a hidden class=anchor aria-hidden=true href=#realsense-node>#</a></h3>
<p>We use Intel’s ROS package for the Realsense camera to publish point clouds and images from the depth camera.
Additionally the realsense node takes care of the following functions: Unwarping the camera image using the camera matrix; Computing the color image projection onto the depth cloud and publishing the combined point cloud.</p>
<h3 id=moveit-node>MoveIt node<a hidden class=anchor aria-hidden=true href=#moveit-node>#</a></h3>
<p>The MoveIt node performs inverse kinematics and path planning, creating a series of steps of joint angles for the Sawyer to move to so that it reaches a desired end effector position without hitting the table.</p>
<h3 id=joint-trajectory-action-server-node>Joint trajectory action server node<a hidden class=anchor aria-hidden=true href=#joint-trajectory-action-server-node>#</a></h3>
<p>Not a node that we made. this node is to allow sawyer to know it’s joints?</p>
<h1 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h1>
<div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden>
<iframe src=https://www.youtube.com/embed/BneLKw1n3bc style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe>
</div>
<hr>
<p>The robot is able to successfully predict where the ball will fall from the table and move its gripper to this spot; our only major issue is the lag in the system, which causes the robot to move a few seconds after the ball has started rolling.</p>
<p>If we were to continue this project, we would implement a different inverse kinematics solver utilizing fewerr joints of the sawyer arm to more efficiently compute the IK problem and more easily follow a path within the constrained set of movements we need.</p>
<hr>
<div style=position:relative;padding-bottom:61.25%;height:0;overflow:hidden>
<iframe src=https://docs.google.com/presentation/d/e/2PACX-1vQupm6jq58CvvSuSTmhdv3kLBg41_WNj4BERRBdKcsQbPSMDoHZ8UJmelcilAyYsgLP-digruSQ76fV/embed style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="Google Slides Presentation"></iframe>
</div>
<h1 id=team>Team<a hidden class=anchor aria-hidden=true href=#team>#</a></h1>
<div style=display:flex;align-items:center;margin-bottom:1em>
<img src=/robert.jpg width=100 height=100 style=float:left;margin-right:2em>
<div>
<p style=margin-bottom:.5em><b>Robert Peltekov</b></p>
<p style=margin-bottom:.5em>
A junior double majoring in EECS and Business at UC Berkeley. Love everything engineering-related from robotics, hardware design to space! In freetime I love to skate both in parks and down hills!
</div>
</div>
<div style=display:flex;align-items:center;margin-bottom:1em>
<img src=/saahil.jpg width=100 height=100 style=float:left;margin-right:2em>
<div>
<p style=margin-bottom:.5em><b>Saahil Parikh</b></p>
<p style=margin-bottom:.5em>
A third year eecs student. I’m a designer and creator. I love taking ideas from conception to fruition.
</div>
</div>
<div style=display:flex;align-items:center;margin-bottom:1em>
<img src=/ryan.jpg width=100 height=100 style=float:left;margin-right:2em>
<div>
<p style=margin-bottom:.5em><b>Ryan Adolf</b></p>
<p style=margin-bottom:.5em>
It’s my third year at Berkeley majoring in EECS. I love engineering, design, art, and just making stuff in general!
</div>
</div>
</div>
<footer class=post-footer>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2021 <a href=http://example.org/>Interceptor</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
</body>
</html>